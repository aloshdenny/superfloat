{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Moonglow (A100 80G)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Kernel is dead"
     ]
    }
   ],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install datasets torch torchvision torchaudio huggingface_hub tqdm psutil GPUtil flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF8 TRAINER (bfloat16) Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from flash_attn import flash_attn_func, flash_attn_kvpacked_func\n",
    "from flash_attn.flash_attention import FlashMHA\n",
    "from flash_attn.bert_padding import unpad_input, pad_input\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import GPUtil\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "!mkdir sf8_llama_quantized\n",
    "\n",
    "def setup_ddp(rank, world_size):\n",
    "    \"\"\"Initialize the distributed environment.\"\"\"\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    # Initialize process group\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    \n",
    "    # Set device for this process\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "\n",
    "def cleanup_ddp():\n",
    "    \"\"\"Clean up the distributed environment.\"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "out_of_range_detected = False\n",
    "\n",
    "def modified_tanh(x):\n",
    "    return torch.tanh(x) * 0.99609375\n",
    "\n",
    "def activation_check_hook(module, input, output):\n",
    "    global out_of_range_detected\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "    else:\n",
    "        output_tensor = output\n",
    "\n",
    "    clamped_output = modified_tanh(output_tensor)\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        return tuple(clamped_output if isinstance(t, torch.Tensor) else t for t in output)\n",
    "    else:\n",
    "        return clamped_output\n",
    "\n",
    "# Flash Attention with MHA\n",
    "# class FlashLlamaAttention(nn.Module):\n",
    "#     \"\"\"Flash Attention implementation for LLaMA\"\"\"\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "#         self.hidden_size = config.hidden_size\n",
    "#         self.num_heads = config.num_attention_heads\n",
    "#         self.head_dim = self.hidden_size // self.num_heads\n",
    "#         self.max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "#         # Initialize Flash attention\n",
    "#         self.flash_attention = FlashMHA(\n",
    "#             embed_dim=self.hidden_size,\n",
    "#             num_heads=self.num_heads,\n",
    "#             bias=False,\n",
    "#             batch_first=True,\n",
    "#             causal=True,\n",
    "#             device=\"cuda\"\n",
    "#         )\n",
    "        \n",
    "#         # Original LLaMA projection layers\n",
    "#         self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "#         self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "#         self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "#         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        \n",
    "#     def forward(self, hidden_states, attention_mask=None, past_key_value=None):\n",
    "#         batch_size, seq_length = hidden_states.shape[:2]\n",
    "        \n",
    "#         # Project queries, keys, and values\n",
    "#         query_states = self.q_proj(hidden_states)\n",
    "#         key_states = self.k_proj(hidden_states)\n",
    "#         value_states = self.v_proj(hidden_states)\n",
    "        \n",
    "#         # Reshape for flash attention\n",
    "#         query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "#         key_states = key_states.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "#         value_states = value_states.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        \n",
    "#         # Handle padding if attention mask is provided\n",
    "#         if attention_mask is not None:\n",
    "#             unpad_masks = attention_mask.bool()\n",
    "#             query_states, indices, cu_seqlens, max_seqlen = unpad_input(query_states, unpad_masks)\n",
    "#             key_states, _, _, _ = unpad_input(key_states, unpad_masks)\n",
    "#             value_states, _, _, _ = unpad_input(value_states, unpad_masks)\n",
    "            \n",
    "#             # Apply Flash Attention\n",
    "#             attn_output = flash_attn_func(\n",
    "#                 query_states, key_states, value_states,\n",
    "#                 cu_seqlens=cu_seqlens,\n",
    "#                 max_seqlen=max_seqlen,\n",
    "#                 causal=True\n",
    "#             )\n",
    "            \n",
    "#             # Pad output back\n",
    "#             attn_output = pad_input(attn_output, indices, batch_size, seq_length)\n",
    "#         else:\n",
    "#             # Direct Flash Attention without padding handling\n",
    "#             attn_output = self.flash_attention(\n",
    "#                 query_states,\n",
    "#                 key_states,\n",
    "#                 value_states\n",
    "#             )\n",
    "        \n",
    "#         attn_output = attn_output.reshape(batch_size, seq_length, self.hidden_size)\n",
    "#         attn_output = self.o_proj(attn_output)\n",
    "        \n",
    "#         # Clamp outputs to SF8 range\n",
    "#         attn_output = torch.clamp(attn_output, min=-0.99609375, max=0.99609375)\n",
    "        \n",
    "#         return attn_output, None\n",
    "\n",
    "# Flash Attention with KV Packed Function\n",
    "class FlashLlamaAttention(nn.Module):\n",
    "    \"\"\"Flash Attention implementation for LLaMA with KV packing optimization\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "        # Original LLaMA projection layers\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, past_key_value=None):\n",
    "        batch_size, seq_length = hidden_states.shape[:2]\n",
    "        \n",
    "        # Project queries, keys, and values\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for flash attention\n",
    "        query_states = query_states.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        key_states = key_states.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        value_states = value_states.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Pack key and value states together\n",
    "        kv_states = torch.stack([key_states, value_states], dim=2)\n",
    "        # Shape becomes: [batch_size, seq_length, 2, num_heads, head_dim]\n",
    "        \n",
    "        # Handle padding if attention mask is provided\n",
    "        if attention_mask is not None:\n",
    "            unpad_masks = attention_mask.bool()\n",
    "            query_states, indices, cu_seqlens, max_seqlen = unpad_input(query_states, unpad_masks)\n",
    "            kv_states, _, _, _ = unpad_input(kv_states, unpad_masks)\n",
    "            \n",
    "            # Apply Flash Attention with packed KV\n",
    "            attn_output = flash_attn_kvpacked_func(\n",
    "                query_states,\n",
    "                kv_states,\n",
    "                cu_seqlens=cu_seqlens,\n",
    "                max_seqlen=max_seqlen,\n",
    "                causal=True,\n",
    "                softmax_scale=1.0 / math.sqrt(self.head_dim)\n",
    "            )\n",
    "            \n",
    "            # Pad output back\n",
    "            attn_output = pad_input(attn_output, indices, batch_size, seq_length)\n",
    "        else:\n",
    "            # If no attention mask, reshape tensors for flash attention\n",
    "            attn_output = flash_attn_kvpacked_func(\n",
    "                query_states,\n",
    "                kv_states,\n",
    "                causal=True,\n",
    "                softmax_scale=1.0 / math.sqrt(self.head_dim)\n",
    "            )\n",
    "        \n",
    "        attn_output = attn_output.reshape(batch_size, seq_length, self.hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        \n",
    "        # Clamp outputs to SF8 range\n",
    "        attn_output = torch.clamp(attn_output, min=-0.99609375, max=0.99609375)\n",
    "        \n",
    "        return attn_output, None\n",
    "    \n",
    "def register_hooks(model):\n",
    "    \"\"\"Register hooks to check activations in the model layers.\"\"\"\n",
    "    hooks = []\n",
    "    for layer in model.model.layers:\n",
    "        hooks.append(layer.register_forward_hook(activation_check_hook))\n",
    "    hooks.append(model.model.norm.register_forward_hook(activation_check_hook))\n",
    "    return hooks\n",
    "\n",
    "def get_memory_usage():\n",
    "    gpu_memory = \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu = GPUtil.getGPUs()[0]\n",
    "        gpu_memory = f\"GPU Memory: {gpu.memoryUsed:.0f}MB/{gpu.memoryTotal:.0f}MB ({gpu.memoryUtil*100:.1f}%)\"\n",
    "    ram_memory = f\"RAM: {psutil.Process().memory_info().rss / 1024 / 1024:.0f}MB\"\n",
    "    return f\"{gpu_memory} | {ram_memory}\"\n",
    "\n",
    "class SF8:\n",
    "    \"\"\"\n",
    "    Super Float 8 (SF8) implementation\n",
    "    1 bit for sign, 7 bits for mantissa\n",
    "    Range: (-1, 1) exclusive\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_sf8(tensor):\n",
    "        return torch.clamp(tensor, min=-0.99609375, max=0.99609375)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_sf8(tensor):\n",
    "        return tensor\n",
    "\n",
    "class SF8Parameter(nn.Parameter):\n",
    "    \"\"\"Custom Parameter class for SF8\"\"\"\n",
    "    def __new__(cls, data=None, requires_grad=True):\n",
    "        tensor = SF8.to_sf8(data) if data is not None else None\n",
    "        return super(SF8Parameter, cls).__new__(cls, tensor, requires_grad)\n",
    "\n",
    "def reclamp_parameters(model):\n",
    "    \"\"\"Clamp all model parameters to SF8 range after conversion.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = torch.clamp(param.data, min=-0.99609375, max=0.99609375)\n",
    "    return model\n",
    "\n",
    "def convert_model_to_sf8_with_flash(model):\n",
    "    \"\"\"Convert model parameters to SF8 format and modify attention layers to use Flash Attention\"\"\"\n",
    "    # Convert parameters to bfloat16 and SF8\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = param.data.to(torch.bfloat16)\n",
    "            sf8_tensor = SF8.to_sf8(param.data)\n",
    "            model._parameters[name] = SF8Parameter(sf8_tensor, requires_grad=param.requires_grad)\n",
    "    \n",
    "    # Replace attention layers with Flash Attention\n",
    "    for layer in model.model.layers:\n",
    "        # Create new Flash Attention module\n",
    "        flash_attention = FlashLlamaAttention(model.config)\n",
    "        \n",
    "        # Copy weights from original attention\n",
    "        flash_attention.q_proj.weight.data = layer.self_attn.q_proj.weight.data\n",
    "        flash_attention.k_proj.weight.data = layer.self_attn.k_proj.weight.data\n",
    "        flash_attention.v_proj.weight.data = layer.self_attn.v_proj.weight.data\n",
    "        flash_attention.o_proj.weight.data = layer.self_attn.o_proj.weight.data\n",
    "        \n",
    "        # Replace attention module\n",
    "        layer.self_attn = flash_attention\n",
    "        \n",
    "        # Add activation clamping\n",
    "        layer.register_forward_hook(activation_check_hook)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class SF8Optimizer(torch.optim.Adam):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.grad.data = torch.clamp(p.grad.data, min=-0.99609375, max=0.99609375)\n",
    "                    super().step(closure)\n",
    "                    with torch.no_grad():\n",
    "                        p.data.clamp_(-0.99609375, 0.99609375)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def prepare_dataset(tokenizer, rank, world_size, max_length=512):\n",
    "    \"\"\"Prepare the dataset with proper tensor formatting\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Create distributed sampler\n",
    "    sampler = DistributedSampler(\n",
    "        tokenized_dataset,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset, sampler\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to properly format tensors\"\"\"\n",
    "    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "def check_sf8_params(model):\n",
    "    \"\"\"Check if all parameters in the model are within the SF8 range.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.data.dim() > 0:  # Check only non-scalar tensors\n",
    "            if not ((param.data >= -0.99609375) & (param.data <= 0.99609375)).all():\n",
    "                print(f\"Parameter {name} out of range!\")\n",
    "                return False\n",
    "    print(\"All parameters are within the SF8 range.\")\n",
    "    return True\n",
    "\n",
    "def train_llama_sf8_ddp(rank, world_size):\n",
    "    \"\"\"Main training function with DDP support\"\"\"\n",
    "    # Initialize distributed setup\n",
    "    setup_ddp(rank, world_size)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir='./')\n",
    "    model = model.to(torch.bfloat16).to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    # Convert model to SF8 with Flash Attention\n",
    "    model = reclamp_parameters(convert_model_to_sf8_with_flash(model))\n",
    "    \n",
    "    # Wrap model with DDP\n",
    "    model = DDP(\n",
    "        model,\n",
    "        device_ids=[rank],\n",
    "        output_device=rank,\n",
    "        find_unused_parameters=False\n",
    "    )\n",
    "    \n",
    "    # Initialize tokenizer only on rank 0 to avoid duplicate downloads\n",
    "    if rank == 0:\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, cache_dir='./')\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Broadcast tokenizer from rank 0 to all processes\n",
    "    dist.barrier()\n",
    "    if rank != 0:\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, cache_dir='./')\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare dataset with distributed sampler\n",
    "    tokenized_dataset, sampler = prepare_dataset(tokenizer, rank, world_size)\n",
    "    \n",
    "    # Create dataloader with distributed sampler\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=4,\n",
    "        sampler=sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = SF8Optimizer(model.parameters())\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    max_grad_norm = 0.99609375\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Only create progress bars on rank 0\n",
    "    if rank == 0:\n",
    "        epoch_pbar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "    else:\n",
    "        epoch_pbar = range(num_epochs)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        # Set epoch for sampler\n",
    "        sampler.set_epoch(epoch)\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Only create batch progress bar on rank 0\n",
    "        if rank == 0:\n",
    "            batch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        else:\n",
    "            batch_pbar = dataloader\n",
    "        \n",
    "        for batch in batch_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = input_ids[:, 1:]\n",
    "            pred = predictions[:, :-1]\n",
    "            mask = attention_mask[:, 1:]\n",
    "            \n",
    "            del outputs, input_ids, attention_mask\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Gather predictions and loss across all processes\n",
    "            dist.all_reduce(loss)\n",
    "            loss = loss / world_size\n",
    "            \n",
    "            correct_pred = ((pred == labels) * mask).sum()\n",
    "            total_pred = mask.sum()\n",
    "            \n",
    "            dist.all_reduce(correct_pred)\n",
    "            dist.all_reduce(total_pred)\n",
    "            \n",
    "            correct_predictions += correct_pred.item()\n",
    "            total_predictions += total_pred.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar on rank 0\n",
    "            if rank == 0:\n",
    "                current_lr = optimizer.param_groups[0]['lr']\n",
    "                batch_pbar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'memory': get_memory_usage(),\n",
    "                    'lr': f\"{current_lr:.2e}\"\n",
    "                })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        # Update progress bar on rank 0\n",
    "        if rank == 0:\n",
    "            epoch_pbar.set_postfix({\n",
    "                'avg_loss': f\"{avg_loss:.4f}\",\n",
    "                'accuracy': f\"{accuracy:.2f}%\",\n",
    "                'memory': get_memory_usage()\n",
    "            })\n",
    "            \n",
    "            # Save checkpoints only on rank 0\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                torch.save(model.module.state_dict(), 'sf8_llama_quantized/sf8_llama_flash_ddp_best_model.pt')\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                torch.save(\n",
    "                    model.module.state_dict(),\n",
    "                    f'sf8_llama_quantized/sf8_llama_flash_ddp_checkpoint_epoch_{epoch+1}.pt'\n",
    "                )\n",
    "    \n",
    "    # Clean up\n",
    "    cleanup_ddp()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to launch distributed training\"\"\"\n",
    "    world_size = torch.cuda.device_count()\n",
    "    if world_size < 2:\n",
    "        raise ValueError(\"Need at least 2 GPUs for distributed training!\")\n",
    "    \n",
    "    print(f\"Starting distributed training on {world_size} GPUs...\")\n",
    "    \n",
    "    # Launch processes\n",
    "    mp.spawn(\n",
    "        train_llama_sf8_ddp,\n",
    "        args=(world_size,),\n",
    "        nprocs=world_size,\n",
    "        join=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF8 TRAINER (bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "!mkdir sf8_llama_quantized\n",
    "\n",
    "out_of_range_detected = False\n",
    "\n",
    "def modified_tanh(x):\n",
    "    return torch.tanh(x) * 0.99609375\n",
    "\n",
    "def activation_check_hook(module, input, output):\n",
    "    global out_of_range_detected\n",
    "    \n",
    "    # If output is a tuple, take the first element\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "    else:\n",
    "        output_tensor = output\n",
    "\n",
    "    # Apply modified tanh to the output\n",
    "    clamped_output = modified_tanh(output_tensor)\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        return tuple(clamped_output if isinstance(t, torch.Tensor) else t for t in output)\n",
    "    else:\n",
    "        return clamped_output\n",
    "\n",
    "def register_hooks(model):\n",
    "    \"\"\"Register hooks to check activations in the model layers.\"\"\"\n",
    "    hooks = []\n",
    "    \n",
    "    # Register hooks for each LlamaDecoderLayer\n",
    "    for layer in model.model.layers:\n",
    "        hooks.append(layer.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    # Also register for the final layer norm\n",
    "    hooks.append(model.model.norm.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    return hooks\n",
    "\n",
    "def get_memory_usage():\n",
    "    gpu_memory = \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu = GPUtil.getGPUs()[0]\n",
    "        gpu_memory = f\"GPU Memory: {gpu.memoryUsed:.0f}MB/{gpu.memoryTotal:.0f}MB ({gpu.memoryUtil*100:.1f}%)\"\n",
    "    ram_memory = f\"RAM: {psutil.Process().memory_info().rss / 1024 / 1024:.0f}MB\"\n",
    "    return f\"{gpu_memory} | {ram_memory}\"\n",
    "\n",
    "class SF8LlamaAttention(nn.Module):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "        \n",
    "        # Clamp attention outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            clamped_outputs = (\n",
    "                torch.clamp(outputs[0], min=-0.99609375, max=0.99609375),\n",
    "                *outputs[1:]\n",
    "            )\n",
    "            return clamped_outputs\n",
    "        return torch.clamp(outputs, min=-0.99609375, max=0.99609375)\n",
    "\n",
    "class SF8:\n",
    "    \"\"\"\n",
    "    Super Float 8 (SF8) implementation\n",
    "    1 bit for sign, 7 bits for mantissa\n",
    "    Range: (-1, 1) exclusive\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_sf8(tensor):\n",
    "        # Clamp values to (-0.99609375, 0.99609375)\n",
    "        return torch.clamp(tensor, min=-0.99609375, max=0.99609375)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_sf8(tensor):\n",
    "        # Convert back to regular float32\n",
    "        return tensor\n",
    "\n",
    "class SF8Parameter(nn.Parameter):\n",
    "    \"\"\"Custom Parameter class for SF8\"\"\"\n",
    "    def __new__(cls, data=None, requires_grad=True):\n",
    "        tensor = SF8.to_sf8(data) if data is not None else None\n",
    "        return super(SF8Parameter, cls).__new__(cls, tensor, requires_grad)\n",
    "    \n",
    "def reclamp_parameters(model):\n",
    "    \"\"\"Clamp all model parameters to SF8 range after conversion.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = torch.clamp(param.data, min=-0.99609375, max=0.99609375)\n",
    "    return model\n",
    "\n",
    "def convert_model_to_sf8(model):\n",
    "    \"\"\"Convert all model parameters to SF8 format and modify attention layers\"\"\"\n",
    "    # First convert parameters to bfloat16 and then to SF8\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = param.data.to(torch.bfloat16)\n",
    "            sf8_tensor = SF8.to_sf8(param.data)\n",
    "#             print(f\"Parameter {name} range: min={sf8_tensor.min():.6f}, max={sf8_tensor.max():.6f}\")\n",
    "            model._parameters[name] = SF8Parameter(sf8_tensor, requires_grad=param.requires_grad)\n",
    "    \n",
    "    # Then modify attention layers to use SF8 attention\n",
    "    for layer in model.model.layers:\n",
    "        # Wrap the original attention module with SF8 attention\n",
    "        original_attention = layer.self_attn\n",
    "        sf8_attention = SF8LlamaAttention()\n",
    "        sf8_attention.__dict__ = original_attention.__dict__.copy()\n",
    "        layer.self_attn = sf8_attention\n",
    "        \n",
    "        # Add activation clamping after feed-forward\n",
    "        layer.register_forward_hook(activation_check_hook)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_sf8_params(model):\n",
    "    \"\"\"Check if all parameters in the model are within the SF8 range.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.data.dim() > 0:  # Check only non-scalar tensors\n",
    "            if not ((param.data >= -0.99609375) & (param.data <= 0.99609375)).all():\n",
    "                print(f\"Parameter {name} out of range!\")\n",
    "                return False\n",
    "    print(\"All parameters are within the SF8 range.\")\n",
    "    return True\n",
    "\n",
    "class SF8Optimizer(torch.optim.Adam):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    # Clamp gradients before the update\n",
    "                    p.grad.data = torch.clamp(p.grad.data, min=-0.99609375, max=0.99609375)\n",
    "                    \n",
    "                    # Regular Adam update\n",
    "                    super().step(closure)\n",
    "                    \n",
    "                    # Clamp parameters after the update\n",
    "                    with torch.no_grad():\n",
    "                        p.data.clamp_(-0.99609375, 0.99609375)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def prepare_dataset(tokenizer, max_length=512):\n",
    "    \"\"\"Prepare the dataset with proper tensor formatting\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to properly format tensors\"\"\"\n",
    "    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "def train_llama_sf8():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir='./')\n",
    "    model = model.to(torch.bfloat16).to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, cache_dir='./')\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Convert model to SF8 and move to device\n",
    "    model = reclamp_parameters(convert_model_to_sf8(model))\n",
    "\n",
    "    # Check if all parameters are in the SF8 range\n",
    "    if not check_sf8_params(model):\n",
    "        raise ValueError(\"Some parameters are out of the SF8 range.\")\n",
    "    \n",
    "    # Save the quantized model before training\n",
    "    torch.save(model.state_dict(), \"sf8_llama_quantized/sf8_llama_quantized.pt\")\n",
    "    tokenizer.save_pretrained(\"sf8_llama_quantized\")\n",
    "    print(\"Saved quantized model\")\n",
    "    \n",
    "    # Clear VRAM\n",
    "    if torch.cuda.is_available():\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared VRAM\")\n",
    "    \n",
    "    # Reload model from saved files\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir = './')\n",
    "    model.load_state_dict(torch.load(\"sf8_llama_quantized/sf8_llama_quantized.pt\"))\n",
    "    model = model.to(device)\n",
    "    print(\"Reloaded model to fresh VRAM\")\n",
    "    \n",
    "    # input_text = \"Sing me a song\"\n",
    "    # inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # global out_of_range_detected\n",
    "    # out_of_range_detected = False\n",
    "    \n",
    "    # hooks = register_hooks(model)\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model.generate(\n",
    "    #         inputs['input_ids'],\n",
    "    #         attention_mask=inputs['attention_mask'],\n",
    "    #         max_length=1024,\n",
    "    #         num_return_sequences=1\n",
    "    #     )\n",
    "    # # Remove hooks after inference\n",
    "    # for hook in hooks:\n",
    "    #     hook.remove()\n",
    "    \n",
    "    # generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(generated_text)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    tokenized_dataset = prepare_dataset(tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = SF8Optimizer(model.parameters())\n",
    "    \n",
    "    # Rest of your training loop remains the same\n",
    "    num_epochs = 100\n",
    "    max_grad_norm = 0.99609375\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Create epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Create batch progress bar\n",
    "        batch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        \n",
    "        for batch in batch_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy (using next token prediction)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = input_ids[:, 1:]  # Shift right to get next token\n",
    "            pred = predictions[:, :-1]  # Remove last prediction\n",
    "            mask = attention_mask[:, 1:] # Adjust mask accordingly\n",
    "            \n",
    "            del outputs, input_ids, attention_mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            correct_predictions += ((pred == labels) * mask).sum().item()\n",
    "            total_predictions += mask.sum().item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            batch_pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'memory': get_memory_usage(),\n",
    "                'lr': f\"{current_lr:.2e}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'avg_loss': f\"{avg_loss:.4f}\",\n",
    "            'accuracy': f\"{accuracy:.2f}%\",\n",
    "            'memory': get_memory_usage()\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint if loss improved\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), 'sf8_llama_quantized/sf8_llama_best_model.pt')\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), f'sf8_llama_quantized/sf8_llama_checkpoint_epoch_{epoch+1}.pt')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    train_llama_sf8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Only run this when pushing files to Hub!\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(\"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\", add_to_git_credential=True)\n",
    "\n",
    "# huggingface-cli upload-large-folder aoxo/llama-3.2-sf8 --repo-type=model /kaggle/working/sf8_llama_quantized --num-workers=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF16 TRAINER (fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "!mkdir sf16_llama_quantized\n",
    "\n",
    "out_of_range_detected = False\n",
    "\n",
    "def modified_tanh(x):\n",
    "    return torch.tanh(x) * 0.9999847412109375\n",
    "\n",
    "def activation_check_hook(module, input, output):\n",
    "    global out_of_range_detected\n",
    "    \n",
    "    # If output is a tuple, take the first element\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "    else:\n",
    "        output_tensor = output\n",
    "\n",
    "    # Apply modified tanh to the output\n",
    "    clamped_output = modified_tanh(output_tensor)\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        return tuple(clamped_output if isinstance(t, torch.Tensor) else t for t in output)\n",
    "    else:\n",
    "        return clamped_output\n",
    "\n",
    "def register_hooks(model):\n",
    "    \"\"\"Register hooks to check activations in the model layers.\"\"\"\n",
    "    hooks = []\n",
    "    \n",
    "    # Register hooks for each LlamaDecoderLayer\n",
    "    for layer in model.model.layers:\n",
    "        hooks.append(layer.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    # Also register for the final layer norm\n",
    "    hooks.append(model.model.norm.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    return hooks\n",
    "\n",
    "def get_memory_usage():\n",
    "    gpu_memory = \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu = GPUtil.getGPUs()[0]\n",
    "        gpu_memory = f\"GPU Memory: {gpu.memoryUsed:.0f}MB/{gpu.memoryTotal:.0f}MB ({gpu.memoryUtil*100:.1f}%)\"\n",
    "    ram_memory = f\"RAM: {psutil.Process().memory_info().rss / 1024 / 1024:.0f}MB\"\n",
    "    return f\"{gpu_memory} | {ram_memory}\"\n",
    "\n",
    "class SF16LlamaAttention(nn.Module):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "        \n",
    "        # Clamp attention outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            clamped_outputs = (\n",
    "                torch.clamp(outputs[0], min=-0.9999847412109375, max=0.9999847412109375),\n",
    "                *outputs[1:]\n",
    "            )\n",
    "            return clamped_outputs\n",
    "        return torch.clamp(outputs, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "\n",
    "class SF16:\n",
    "    \"\"\"\n",
    "    Super Float 16 (SF16) implementation\n",
    "    1 bit for sign, 7 bits for mantissa\n",
    "    Range: (-1, 1) exclusive\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_sf16(tensor):\n",
    "        # Clamp values to (-0.9999847412109375, 0.9999847412109375)\n",
    "        return torch.clamp(tensor, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_sf16(tensor):\n",
    "        # Convert back to regular float32\n",
    "        return tensor\n",
    "\n",
    "class SF16Parameter(nn.Parameter):\n",
    "    \"\"\"Custom Parameter class for SF16\"\"\"\n",
    "    def __new__(cls, data=None, requires_grad=True):\n",
    "        tensor = SF16.to_sf16(data) if data is not None else None\n",
    "        return super(SF16Parameter, cls).__new__(cls, tensor, requires_grad)\n",
    "    \n",
    "def reclamp_parameters(model):\n",
    "    \"\"\"Clamp all model parameters to SF16 range after conversion.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = torch.clamp(param.data, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "    return model\n",
    "\n",
    "def convert_model_to_sf16(model):\n",
    "    \"\"\"Convert all model parameters to SF16 format and modify attention layers\"\"\"\n",
    "    # Convert parameters to SF16\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            sf16_tensor = SF16.to_sf16(param.data)\n",
    "#             print(f\"Parameter {name} range: min={sf16_tensor.min():.6f}, max={sf16_tensor.max():.6f}\")\n",
    "            model._parameters[name] = SF16Parameter(sf16_tensor, requires_grad=param.requires_grad)\n",
    "    \n",
    "    # Then modify attention layers to use SF16 attention\n",
    "    for layer in model.model.layers:\n",
    "        # Wrap the original attention module with SF16 attention\n",
    "        original_attention = layer.self_attn\n",
    "        sf16_attention = SF16LlamaAttention()\n",
    "        sf16_attention.__dict__ = original_attention.__dict__.copy()\n",
    "        layer.self_attn = sf16_attention\n",
    "        \n",
    "        # Add activation clamping after feed-forward\n",
    "        layer.register_forward_hook(activation_check_hook)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_sf16_params(model):\n",
    "    \"\"\"Check if all parameters in the model are within the SF16 range.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.data.dim() > 0:  # Check only non-scalar tensors\n",
    "            if not ((param.data >= -0.9999847412109375) & (param.data <= 0.9999847412109375)).all():\n",
    "                print(f\"Parameter {name} out of range!\")\n",
    "                return False\n",
    "    print(\"All parameters are within the SF16 range.\")\n",
    "    return True\n",
    "\n",
    "class SF16Optimizer(torch.optim.Adam):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    # Clamp gradients before the update\n",
    "                    p.grad.data = torch.clamp(p.grad.data, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "                    \n",
    "                    # Regular Adam update\n",
    "                    super().step(closure)\n",
    "                    \n",
    "                    # Clamp parameters after the update\n",
    "                    with torch.no_grad():\n",
    "                        p.data.clamp_(-0.9999847412109375, 0.9999847412109375)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def prepare_dataset(tokenizer, max_length=512):\n",
    "    \"\"\"Prepare the dataset with proper tensor formatting\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to properly format tensors\"\"\"\n",
    "    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "def train_llama_sf16():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir='./').to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, cache_dir='./')\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Convert model to SF16 and move to device\n",
    "    model = reclamp_parameters(convert_model_to_sf16(model))\n",
    "\n",
    "    # Check if all parameters are in the SF16 range\n",
    "    if not check_sf16_params(model):\n",
    "        raise ValueError(\"Some parameters are out of the SF16 range.\")\n",
    "    \n",
    "    # Save the quantized model before training\n",
    "    torch.save(model.state_dict(), \"sf16_llama_quantized/sf16_llama_quantized.pt\")\n",
    "    tokenizer.save_pretrained(\"sf16_llama_quantized\")\n",
    "    print(\"Saved quantized model\")\n",
    "    \n",
    "    # Clear VRAM\n",
    "    if torch.cuda.is_available():\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared VRAM\")\n",
    "    \n",
    "    # Reload model from saved files\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir = './')\n",
    "    model.load_state_dict(torch.load(\"sf16_llama_quantized/sf16_llama_quantized.pt\"))\n",
    "    model = model.to(device)\n",
    "    print(\"Reloaded model to fresh VRAM\")\n",
    "    \n",
    "    input_text = \"Sing me a song\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    global out_of_range_detected\n",
    "    out_of_range_detected = False\n",
    "    \n",
    "    hooks = register_hooks(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=1024,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    # Remove hooks after inference\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    tokenized_dataset = prepare_dataset(tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = SF16Optimizer(model.parameters())\n",
    "    \n",
    "    # Rest of your training loop remains the same\n",
    "    num_epochs = 100\n",
    "    max_grad_norm = 0.9999847412109375\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Create epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Create batch progress bar\n",
    "        batch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        \n",
    "        for batch in batch_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy (using next token prediction)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = input_ids[:, 1:]  # Shift right to get next token\n",
    "            pred = predictions[:, :-1]  # Remove last prediction\n",
    "            mask = attention_mask[:, 1:] # Adjust mask accordingly\n",
    "            \n",
    "            del outputs, input_ids, attention_mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            correct_predictions += ((pred == labels) * mask).sum().item()\n",
    "            total_predictions += mask.sum().item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            batch_pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'memory': get_memory_usage(),\n",
    "                'lr': f\"{current_lr:.2e}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'avg_loss': f\"{avg_loss:.4f}\",\n",
    "            'accuracy': f\"{accuracy:.2f}%\",\n",
    "            'memory': get_memory_usage()\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint if loss improved\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), 'sf16_llama_quantized/sf16_llama_best_model.pt')\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), f'sf16_llama_quantized/sf16_llama_checkpoint_epoch_{epoch+1}.pt')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    train_llama_sf16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Only run this when pushing files to Hub!\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(\"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\", add_to_git_credential=True)\n",
    "\n",
    "# huggingface-cli upload-large-folder aoxo/llama-3.2-sf16 --repo-type=model /kaggle/working/sf16_llama_quantized --num-workers=16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Moonglow (A100 80G)",
   "language": "python",
   "name": "moonglow-a100-80gb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
