{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.6)\n",
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.1.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m170.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m171.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m241.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m158.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m136.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.4/318.4 kB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7393 sha256=7cf0042526aea4497afdcc0ca00125ea4123093e176980b756d85d3375ca056e\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: pytz, GPUtil, xxhash, tzdata, requests, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed GPUtil-1.4.0 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.2 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.2.0 pyarrow-18.0.0 pytz-2024.2 requests-2.32.3 tzdata-2024.2 xxhash-3.5.0 yarl-1.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install datasets torch torchvision torchaudio huggingface_hub tqdm psutil GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF8 TRAINER (bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "!mkdir sf8_llama_quantized\n",
    "\n",
    "out_of_range_detected = False\n",
    "\n",
    "def modified_tanh(x):\n",
    "    return torch.tanh(x) * 0.99609375\n",
    "\n",
    "def activation_check_hook(module, input, output):\n",
    "    global out_of_range_detected\n",
    "    \n",
    "    # If output is a tuple, take the first element\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "    else:\n",
    "        output_tensor = output\n",
    "\n",
    "    # Apply modified tanh to the output\n",
    "    clamped_output = modified_tanh(output_tensor)\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        return tuple(clamped_output if isinstance(t, torch.Tensor) else t for t in output)\n",
    "    else:\n",
    "        return clamped_output\n",
    "\n",
    "def register_hooks(model):\n",
    "    \"\"\"Register hooks to check activations in the model layers.\"\"\"\n",
    "    hooks = []\n",
    "    \n",
    "    # Register hooks for each LlamaDecoderLayer\n",
    "    for layer in model.model.layers:\n",
    "        hooks.append(layer.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    # Also register for the final layer norm\n",
    "    hooks.append(model.model.norm.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    return hooks\n",
    "\n",
    "def get_memory_usage():\n",
    "    gpu_memory = \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu = GPUtil.getGPUs()[0]\n",
    "        gpu_memory = f\"GPU Memory: {gpu.memoryUsed:.0f}MB/{gpu.memoryTotal:.0f}MB ({gpu.memoryUtil*100:.1f}%)\"\n",
    "    ram_memory = f\"RAM: {psutil.Process().memory_info().rss / 1024 / 1024:.0f}MB\"\n",
    "    return f\"{gpu_memory} | {ram_memory}\"\n",
    "\n",
    "class SF8LlamaAttention(nn.Module):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "        \n",
    "        # Clamp attention outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            clamped_outputs = (\n",
    "                torch.clamp(outputs[0], min=-0.99609375, max=0.99609375),\n",
    "                *outputs[1:]\n",
    "            )\n",
    "            return clamped_outputs\n",
    "        return torch.clamp(outputs, min=-0.99609375, max=0.99609375)\n",
    "\n",
    "class SF8:\n",
    "    \"\"\"\n",
    "    Super Float 8 (SF8) implementation\n",
    "    1 bit for sign, 7 bits for mantissa\n",
    "    Range: (-1, 1) exclusive\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_sf8(tensor):\n",
    "        # Clamp values to (-0.99609375, 0.99609375)\n",
    "        return torch.clamp(tensor, min=-0.99609375, max=0.99609375)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_sf8(tensor):\n",
    "        # Convert back to regular float32\n",
    "        return tensor\n",
    "\n",
    "class SF8Parameter(nn.Parameter):\n",
    "    \"\"\"Custom Parameter class for SF8\"\"\"\n",
    "    def __new__(cls, data=None, requires_grad=True):\n",
    "        tensor = SF8.to_sf8(data) if data is not None else None\n",
    "        return super(SF8Parameter, cls).__new__(cls, tensor, requires_grad)\n",
    "    \n",
    "def reclamp_parameters(model):\n",
    "    \"\"\"Clamp all model parameters to SF8 range after conversion.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = torch.clamp(param.data, min=-0.99609375, max=0.99609375)\n",
    "    return model\n",
    "\n",
    "def convert_model_to_sf8(model):\n",
    "    \"\"\"Convert all model parameters to SF8 format and modify attention layers\"\"\"\n",
    "    # First convert parameters to bfloat16 and then to SF8\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = param.data.to(torch.bfloat16)\n",
    "            sf8_tensor = SF8.to_sf8(param.data)\n",
    "#             print(f\"Parameter {name} range: min={sf8_tensor.min():.6f}, max={sf8_tensor.max():.6f}\")\n",
    "            model._parameters[name] = SF8Parameter(sf8_tensor, requires_grad=param.requires_grad)\n",
    "    \n",
    "    # Then modify attention layers to use SF8 attention\n",
    "    for layer in model.model.layers:\n",
    "        # Wrap the original attention module with SF8 attention\n",
    "        original_attention = layer.self_attn\n",
    "        sf8_attention = SF8LlamaAttention()\n",
    "        sf8_attention.__dict__ = original_attention.__dict__.copy()\n",
    "        layer.self_attn = sf8_attention\n",
    "        \n",
    "        # Add activation clamping after feed-forward\n",
    "        layer.register_forward_hook(activation_check_hook)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_sf8_params(model):\n",
    "    \"\"\"Check if all parameters in the model are within the SF8 range.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.data.dim() > 0:  # Check only non-scalar tensors\n",
    "            if not ((param.data >= -0.99609375) & (param.data <= 0.99609375)).all():\n",
    "                print(f\"Parameter {name} out of range!\")\n",
    "                return False\n",
    "    print(\"All parameters are within the SF8 range.\")\n",
    "    return True\n",
    "\n",
    "class SF8Optimizer(torch.optim.Adam):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    # Clamp gradients before the update\n",
    "                    p.grad.data = torch.clamp(p.grad.data, min=-0.99609375, max=0.99609375)\n",
    "                    \n",
    "                    # Regular Adam update\n",
    "                    super().step(closure)\n",
    "                    \n",
    "                    # Clamp parameters after the update\n",
    "                    with torch.no_grad():\n",
    "                        p.data.clamp_(-0.99609375, 0.99609375)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def prepare_dataset(tokenizer, max_length=512):\n",
    "    \"\"\"Prepare the dataset with proper tensor formatting\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to properly format tensors\"\"\"\n",
    "    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "def train_llama_sf8():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir='./')\n",
    "    model = model.to(torch.bfloat16).to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, cache_dir='./')\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Convert model to SF8 and move to device\n",
    "    model = reclamp_parameters(convert_model_to_sf8(model))\n",
    "\n",
    "    # Check if all parameters are in the SF8 range\n",
    "    if not check_sf8_params(model):\n",
    "        raise ValueError(\"Some parameters are out of the SF8 range.\")\n",
    "    \n",
    "    # Save the quantized model before training\n",
    "    torch.save(model.state_dict(), \"sf8_llama_quantized/sf8_llama_quantized.pt\")\n",
    "    tokenizer.save_pretrained(\"sf8_llama_quantized\")\n",
    "    print(\"Saved quantized model\")\n",
    "    \n",
    "    # Clear VRAM\n",
    "    if torch.cuda.is_available():\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared VRAM\")\n",
    "    \n",
    "    # Reload model from saved files\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir = './')\n",
    "    model.load_state_dict(torch.load(\"sf8_llama_quantized/sf8_llama_quantized.pt\"))\n",
    "    model = model.to(device)\n",
    "    print(\"Reloaded model to fresh VRAM\")\n",
    "    \n",
    "    # input_text = \"Sing me a song\"\n",
    "    # inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # global out_of_range_detected\n",
    "    # out_of_range_detected = False\n",
    "    \n",
    "    # hooks = register_hooks(model)\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model.generate(\n",
    "    #         inputs['input_ids'],\n",
    "    #         attention_mask=inputs['attention_mask'],\n",
    "    #         max_length=1024,\n",
    "    #         num_return_sequences=1\n",
    "    #     )\n",
    "    # # Remove hooks after inference\n",
    "    # for hook in hooks:\n",
    "    #     hook.remove()\n",
    "    \n",
    "    # generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(generated_text)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    tokenized_dataset = prepare_dataset(tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = SF8Optimizer(model.parameters())\n",
    "    \n",
    "    # Rest of your training loop remains the same\n",
    "    num_epochs = 100\n",
    "    max_grad_norm = 0.99609375\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Create epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Create batch progress bar\n",
    "        batch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        \n",
    "        for batch in batch_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy (using next token prediction)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = input_ids[:, 1:]  # Shift right to get next token\n",
    "            pred = predictions[:, :-1]  # Remove last prediction\n",
    "            mask = attention_mask[:, 1:] # Adjust mask accordingly\n",
    "            \n",
    "            del outputs, input_ids, attention_mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            correct_predictions += ((pred == labels) * mask).sum().item()\n",
    "            total_predictions += mask.sum().item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            batch_pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'memory': get_memory_usage(),\n",
    "                'lr': f\"{current_lr:.2e}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'avg_loss': f\"{avg_loss:.4f}\",\n",
    "            'accuracy': f\"{accuracy:.2f}%\",\n",
    "            'memory': get_memory_usage()\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint if loss improved\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), 'sf8_llama_quantized/sf8_llama_best_model.pt')\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), f'sf8_llama_quantized/sf8_llama_checkpoint_epoch_{epoch+1}.pt')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    train_llama_sf8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Only run this when pushing files to Hub!\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(\"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\", add_to_git_credential=True)\n",
    "\n",
    "# huggingface-cli upload-large-folder aoxo/llama-3.2-sf8 --repo-type=model /kaggle/working/sf8_llama_quantized --num-workers=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SF16 TRAINER (fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "!mkdir sf16_llama_quantized\n",
    "\n",
    "out_of_range_detected = False\n",
    "\n",
    "def modified_tanh(x):\n",
    "    return torch.tanh(x) * 0.9999847412109375\n",
    "\n",
    "def activation_check_hook(module, input, output):\n",
    "    global out_of_range_detected\n",
    "    \n",
    "    # If output is a tuple, take the first element\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]\n",
    "    else:\n",
    "        output_tensor = output\n",
    "\n",
    "    # Apply modified tanh to the output\n",
    "    clamped_output = modified_tanh(output_tensor)\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        return tuple(clamped_output if isinstance(t, torch.Tensor) else t for t in output)\n",
    "    else:\n",
    "        return clamped_output\n",
    "\n",
    "def register_hooks(model):\n",
    "    \"\"\"Register hooks to check activations in the model layers.\"\"\"\n",
    "    hooks = []\n",
    "    \n",
    "    # Register hooks for each LlamaDecoderLayer\n",
    "    for layer in model.model.layers:\n",
    "        hooks.append(layer.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    # Also register for the final layer norm\n",
    "    hooks.append(model.model.norm.register_forward_hook(activation_check_hook))\n",
    "    \n",
    "    return hooks\n",
    "\n",
    "def get_memory_usage():\n",
    "    gpu_memory = \"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu = GPUtil.getGPUs()[0]\n",
    "        gpu_memory = f\"GPU Memory: {gpu.memoryUsed:.0f}MB/{gpu.memoryTotal:.0f}MB ({gpu.memoryUtil*100:.1f}%)\"\n",
    "    ram_memory = f\"RAM: {psutil.Process().memory_info().rss / 1024 / 1024:.0f}MB\"\n",
    "    return f\"{gpu_memory} | {ram_memory}\"\n",
    "\n",
    "class SF16LlamaAttention(nn.Module):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "        \n",
    "        # Clamp attention outputs\n",
    "        if isinstance(outputs, tuple):\n",
    "            clamped_outputs = (\n",
    "                torch.clamp(outputs[0], min=-0.9999847412109375, max=0.9999847412109375),\n",
    "                *outputs[1:]\n",
    "            )\n",
    "            return clamped_outputs\n",
    "        return torch.clamp(outputs, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "\n",
    "class SF16:\n",
    "    \"\"\"\n",
    "    Super Float 16 (SF16) implementation\n",
    "    1 bit for sign, 7 bits for mantissa\n",
    "    Range: (-1, 1) exclusive\n",
    "    \"\"\"\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_sf16(tensor):\n",
    "        # Clamp values to (-0.9999847412109375, 0.9999847412109375)\n",
    "        return torch.clamp(tensor, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_sf16(tensor):\n",
    "        # Convert back to regular float32\n",
    "        return tensor\n",
    "\n",
    "class SF16Parameter(nn.Parameter):\n",
    "    \"\"\"Custom Parameter class for SF16\"\"\"\n",
    "    def __new__(cls, data=None, requires_grad=True):\n",
    "        tensor = SF16.to_sf16(data) if data is not None else None\n",
    "        return super(SF16Parameter, cls).__new__(cls, tensor, requires_grad)\n",
    "    \n",
    "def reclamp_parameters(model):\n",
    "    \"\"\"Clamp all model parameters to SF16 range after conversion.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            param.data = torch.clamp(param.data, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "    return model\n",
    "\n",
    "def convert_model_to_sf16(model):\n",
    "    \"\"\"Convert all model parameters to SF16 format and modify attention layers\"\"\"\n",
    "    # Convert parameters to SF16\n",
    "    for name, param in model.named_parameters():\n",
    "        if isinstance(param, nn.Parameter):\n",
    "            sf16_tensor = SF16.to_sf16(param.data)\n",
    "#             print(f\"Parameter {name} range: min={sf16_tensor.min():.6f}, max={sf16_tensor.max():.6f}\")\n",
    "            model._parameters[name] = SF16Parameter(sf16_tensor, requires_grad=param.requires_grad)\n",
    "    \n",
    "    # Then modify attention layers to use SF16 attention\n",
    "    for layer in model.model.layers:\n",
    "        # Wrap the original attention module with SF16 attention\n",
    "        original_attention = layer.self_attn\n",
    "        sf16_attention = SF16LlamaAttention()\n",
    "        sf16_attention.__dict__ = original_attention.__dict__.copy()\n",
    "        layer.self_attn = sf16_attention\n",
    "        \n",
    "        # Add activation clamping after feed-forward\n",
    "        layer.register_forward_hook(activation_check_hook)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def check_sf16_params(model):\n",
    "    \"\"\"Check if all parameters in the model are within the SF16 range.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.data.dim() > 0:  # Check only non-scalar tensors\n",
    "            if not ((param.data >= -0.9999847412109375) & (param.data <= 0.9999847412109375)).all():\n",
    "                print(f\"Parameter {name} out of range!\")\n",
    "                return False\n",
    "    print(\"All parameters are within the SF16 range.\")\n",
    "    return True\n",
    "\n",
    "class SF16Optimizer(torch.optim.Adam):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    # Clamp gradients before the update\n",
    "                    p.grad.data = torch.clamp(p.grad.data, min=-0.9999847412109375, max=0.9999847412109375)\n",
    "                    \n",
    "                    # Regular Adam update\n",
    "                    super().step(closure)\n",
    "                    \n",
    "                    # Clamp parameters after the update\n",
    "                    with torch.no_grad():\n",
    "                        p.data.clamp_(-0.9999847412109375, 0.9999847412109375)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "def prepare_dataset(tokenizer, max_length=512):\n",
    "    \"\"\"Prepare the dataset with proper tensor formatting\"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to properly format tensors\"\"\"\n",
    "    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "def train_llama_sf16():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir='./').to(device)\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, cache_dir='./')\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Convert model to SF16 and move to device\n",
    "    model = reclamp_parameters(convert_model_to_sf16(model))\n",
    "\n",
    "    # Check if all parameters are in the SF16 range\n",
    "    if not check_sf16_params(model):\n",
    "        raise ValueError(\"Some parameters are out of the SF16 range.\")\n",
    "    \n",
    "    # Save the quantized model before training\n",
    "    torch.save(model.state_dict(), \"sf16_llama_quantized/sf16_llama_quantized.pt\")\n",
    "    tokenizer.save_pretrained(\"sf16_llama_quantized\")\n",
    "    print(\"Saved quantized model\")\n",
    "    \n",
    "    # Clear VRAM\n",
    "    if torch.cuda.is_available():\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Cleared VRAM\")\n",
    "    \n",
    "    # Reload model from saved files\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, cache_dir = './')\n",
    "    model.load_state_dict(torch.load(\"sf16_llama_quantized/sf16_llama_quantized.pt\"))\n",
    "    model = model.to(device)\n",
    "    print(\"Reloaded model to fresh VRAM\")\n",
    "    \n",
    "    input_text = \"Sing me a song\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    global out_of_range_detected\n",
    "    out_of_range_detected = False\n",
    "    \n",
    "    hooks = register_hooks(model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=1024,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    # Remove hooks after inference\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    tokenized_dataset = prepare_dataset(tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        tokenized_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = SF16Optimizer(model.parameters())\n",
    "    \n",
    "    # Rest of your training loop remains the same\n",
    "    num_epochs = 100\n",
    "    max_grad_norm = 0.9999847412109375\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Create epoch progress bar\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Create batch progress bar\n",
    "        batch_pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", position=1, leave=False)\n",
    "        \n",
    "        for batch in batch_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate accuracy (using next token prediction)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            labels = input_ids[:, 1:]  # Shift right to get next token\n",
    "            pred = predictions[:, :-1]  # Remove last prediction\n",
    "            mask = attention_mask[:, 1:] # Adjust mask accordingly\n",
    "            \n",
    "            del outputs, input_ids, attention_mask\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            correct_predictions += ((pred == labels) * mask).sum().item()\n",
    "            total_predictions += mask.sum().item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Update batch progress bar\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            batch_pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'memory': get_memory_usage(),\n",
    "                'lr': f\"{current_lr:.2e}\"\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        accuracy = 100 * correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        \n",
    "        # Update epoch progress bar\n",
    "        epoch_pbar.set_postfix({\n",
    "            'avg_loss': f\"{avg_loss:.4f}\",\n",
    "            'accuracy': f\"{accuracy:.2f}%\",\n",
    "            'memory': get_memory_usage()\n",
    "        })\n",
    "        \n",
    "        # Save checkpoint if loss improved\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), 'sf16_llama_quantized/sf16_llama_best_model.pt')\n",
    "        \n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save(model.state_dict(), f'sf16_llama_quantized/sf16_llama_checkpoint_epoch_{epoch+1}.pt')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    train_llama_sf16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Only run this when pushing files to Hub!\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(\"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\", add_to_git_credential=True)\n",
    "\n",
    "# huggingface-cli upload-large-folder aoxo/llama-3.2-sf16 --repo-type=model /kaggle/working/sf16_llama_quantized --num-workers=16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Moonglow (A40)",
   "language": "python",
   "name": "moonglow-a40"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
