{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_layers_in_range(model, lower_bound, upper_bound):\n",
    "    layers_in_range = []\n",
    "    layers_out_of_range = []\n",
    "    all_layers = []  # To keep track of the order in which layers are encountered\n",
    "\n",
    "    for name, param in tqdm(model.named_parameters(), desc=\"Analyzing parameters\"):\n",
    "        if param.requires_grad:\n",
    "            param_cpu = param.detach().cpu().float()\n",
    "            \n",
    "            # Check if all parameters in the layer are within the specified range\n",
    "            in_range = ((param_cpu >= lower_bound) & (param_cpu <= upper_bound)).all().item()\n",
    "            \n",
    "            if in_range:\n",
    "                layers_in_range.append(name)\n",
    "            else:\n",
    "                layers_out_of_range.append(name)\n",
    "            \n",
    "            # Record the order of layers\n",
    "            all_layers.append((name, in_range))\n",
    "\n",
    "    return layers_in_range, layers_out_of_range, all_layers\n",
    "\n",
    "def print_layer_inlier_outlier(all_layers):\n",
    "    print(\"\\nLayer Status Report:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Print information for each layer in the order they were encountered\n",
    "    for layer_name, in_range in all_layers:\n",
    "        if in_range:\n",
    "            print(f\"{layer_name}: Inlier (All parameters within range)\")\n",
    "        else:\n",
    "            print(f\"{layer_name}: Outlier (Some parameters out of range)\")\n",
    "\n",
    "def plot_layer_status(all_layers, output_path):\n",
    "    # Extract statuses\n",
    "    statuses = [1 if in_range else 0 for _, in_range in all_layers]\n",
    "\n",
    "    plt.figure(figsize=(20, 10))  # Adjusted figure size for better readability\n",
    "    bar_width = 0.8  # Set a fixed width for the bars\n",
    "    x = range(len(all_layers))  # X-axis positions for the bars\n",
    "\n",
    "    # Plot the bars\n",
    "    plt.bar(x, [1] * len(all_layers), color=['green' if status == 1 else 'red' for status in statuses], width=bar_width)\n",
    "\n",
    "    # Set the x-ticks to be numbered (0, 1, 2, ..., len(all_layers)-1)\n",
    "    plt.xticks(x[::10], [i for i in range(len(all_layers))][::10], rotation=90)  # Show every 10th tick for clarity\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Status')\n",
    "    plt.title('Layer Parameter Status')\n",
    "\n",
    "    # Remove everything under x-axis\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    plt.gca().xaxis.set_ticks_position('none')\n",
    "    plt.gca().yaxis.set_ticks_position('none')\n",
    "\n",
    "    # Adjust layout to make room for labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(output_path, bbox_inches='tight')  # Save the plot to the specified file path\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"stabilityai/japanese-stablelm-base-beta-7b\"  # Update with the correct model name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "lower_bound, upper_bound = -1, 1\n",
    "\n",
    "layers_in_range, layers_out_of_range, all_layers = analyze_layers_in_range(model, lower_bound, upper_bound)\n",
    "\n",
    "print(f\"Layers with all parameters in range: {len(layers_in_range)}\")\n",
    "print(f\"Layers with any parameters out of range: {len(layers_out_of_range)}\")\n",
    "\n",
    "# Print the layer status report in the order encountered\n",
    "print_layer_inlier_outlier(all_layers)\n",
    "\n",
    "# Define the output path (adjust this path to a location on your local system)\n",
    "output_path = \"layer_status_plot_stablelm.png\"  # Path to save the plot\n",
    "\n",
    "# Plot the layer status bar graph and save it\n",
    "plot_layer_status(all_layers, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_layers_in_range(model, lower_bound, upper_bound):\n",
    "    layers_in_range = []\n",
    "    layers_out_of_range = []\n",
    "    all_layers = []  # To keep track of the order in which layers are encountered\n",
    "\n",
    "    for name, param in tqdm(model.named_parameters(), desc=\"Analyzing parameters\"):\n",
    "        if param.requires_grad:\n",
    "            param_cpu = param.detach().cpu().float()\n",
    "            \n",
    "            # Check if all parameters in the layer are within the specified range\n",
    "            in_range = ((param_cpu >= lower_bound) & (param_cpu <= upper_bound)).all().item()\n",
    "            \n",
    "            if in_range:\n",
    "                layers_in_range.append(name)\n",
    "            else:\n",
    "                layers_out_of_range.append(name)\n",
    "            \n",
    "            # Record the order of layers\n",
    "            all_layers.append((name, in_range))\n",
    "\n",
    "    return layers_in_range, layers_out_of_range, all_layers\n",
    "\n",
    "def print_layer_inlier_outlier(all_layers):\n",
    "    print(\"\\nLayer Status Report:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Print information for each layer in the order they were encountered\n",
    "    for layer_name, in_range in all_layers:\n",
    "        if in_range:\n",
    "            print(f\"{layer_name}: Inlier (All parameters within range)\")\n",
    "        else:\n",
    "            print(f\"{layer_name}: Outlier (Some parameters out of range)\")\n",
    "\n",
    "def plot_layer_status(all_layers, output_path):\n",
    "    # Extract statuses\n",
    "    statuses = [1 if in_range else 0 for _, in_range in all_layers]\n",
    "\n",
    "    plt.figure(figsize=(20, 10))  # Adjusted figure size for better readability\n",
    "    bar_width = 0.8  # Set a fixed width for the bars\n",
    "    x = range(len(all_layers))  # X-axis positions for the bars\n",
    "\n",
    "    # Plot the bars\n",
    "    plt.bar(x, [1] * len(all_layers), color=['green' if status == 1 else 'red' for status in statuses], width=bar_width)\n",
    "\n",
    "    # Set the x-ticks to be numbered (0, 1, 2, ..., len(all_layers)-1)\n",
    "    plt.xticks(x[::10], [i for i in range(len(all_layers))][::10], rotation=90)  # Show every 10th tick for clarity\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Status')\n",
    "    plt.title('Layer Parameter Status')\n",
    "\n",
    "    # Remove everything under x-axis\n",
    "    plt.gca().spines['bottom'].set_visible(False)\n",
    "    plt.gca().spines['left'].set_visible(False)\n",
    "    plt.gca().xaxis.set_ticks_position('none')\n",
    "    plt.gca().yaxis.set_ticks_position('none')\n",
    "\n",
    "    # Adjust layout to make room for labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(output_path, bbox_inches='tight')  # Save the plot to the specified file path\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"openbmb/MiniCPM-V-2\"  # Update with the correct model name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True).to(device)\n",
    "\n",
    "lower_bound, upper_bound = -1, 1\n",
    "\n",
    "layers_in_range, layers_out_of_range, all_layers = analyze_layers_in_range(model, lower_bound, upper_bound)\n",
    "\n",
    "print(f\"Layers with all parameters in range: {len(layers_in_range)}\")\n",
    "print(f\"Layers with any parameters out of range: {len(layers_out_of_range)}\")\n",
    "\n",
    "# Print the layer status report in the order encountered\n",
    "print_layer_inlier_outlier(all_layers)\n",
    "\n",
    "# Define the output path (adjust this path to a location on your local system)\n",
    "output_path = \"layer_status_plot_stablelm.png\"  # Path to save the plot\n",
    "\n",
    "# Plot the layer status bar graph and save it\n",
    "plot_layer_status(all_layers, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov7.git\n",
    "!pip install -r yolov7/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt -P yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt -P yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt -P yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt -P yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt -P yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt -P yolov7\n",
    "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt -P yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VERIFY IF MODEL IS QUANTIZED FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "SF16_MAX = 0.999969482421875\n",
    "SF16_MIN = -0.999969482421875\n",
    "SF8_MAX = 0.9921875\n",
    "SF8_MIN = -0.9921875\n",
    "\n",
    "def is_in_sf16_range(tensor):\n",
    "    return torch.all((tensor >= SF16_MIN) & (tensor <= SF16_MAX))\n",
    "\n",
    "def quantize_to_sf16(tensor):\n",
    "    # Clip the tensor to the SF16 range\n",
    "    tensor = torch.clamp(tensor, SF16_MIN, SF16_MAX)\n",
    "    \n",
    "    # Apply the scaling to fit into 15-bit mantissa\n",
    "    scaled = (tensor - SF16_MIN) / (SF16_MAX - SF16_MIN) * (2**15 - 1)\n",
    "    \n",
    "    # Round to nearest integer for mantissa\n",
    "    rounded = torch.round(scaled)\n",
    "    \n",
    "    # Convert back to SF16 range by scaling it again\n",
    "    quantized = rounded / (2**15 - 1) * (SF16_MAX - SF16_MIN) + SF16_MIN\n",
    "    \n",
    "    return quantized\n",
    "\n",
    "def apply_selective_sf16_quantization(model):\n",
    "    total_params = 0\n",
    "    quantized_params = 0\n",
    "    memory_usage = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if is_in_sf16_range(param.data):\n",
    "            param.data = quantize_to_sf16(param.data)\n",
    "            quantized_params += param.numel()\n",
    "            memory_usage += param.numel() * 16 / 8  # 16 bits per parameter\n",
    "        else:\n",
    "            memory_usage += param.numel() * 32 / 8  # 32 bits for non-quantized parameters\n",
    "\n",
    "    memory_usage_mb = memory_usage / (1024 * 1024)\n",
    "    return total_params, quantized_params, memory_usage_mb\n",
    "\n",
    "def verify_selective_sf16_quantization(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if is_in_sf16_range(param.data):\n",
    "            unique_values = torch.unique(param)\n",
    "            if len(unique_values) > 65536:\n",
    "                print(f\"Warning: Quantized parameter {name} has more than 65536 unique values\")\n",
    "            else:\n",
    "                print(f\"Layer {name} is correctly quantized to SF16\")\n",
    "        else:\n",
    "            print(f\"Layer {name} is not quantized (outside SF16 range)\")\n",
    "            \n",
    "def is_in_sf8_range(tensor):\n",
    "    return torch.all((tensor >= SF8_MIN) & (tensor <= SF8_MAX))\n",
    "\n",
    "def quantize_to_sf8(tensor):\n",
    "    # Clip the tensor to the SF8 range\n",
    "    tensor = torch.clamp(tensor, SF8_MIN, SF8_MAX)\n",
    "    \n",
    "    # Apply the scaling to fit into 7-bit mantissa\n",
    "    scaled = (tensor - SF8_MIN) / (SF8_MAX - SF8_MIN) * (2**7 - 1)\n",
    "    \n",
    "    # Round to nearest integer for mantissa\n",
    "    rounded = torch.round(scaled)\n",
    "    \n",
    "    # Convert back to SF8 range by scaling it again\n",
    "    quantized = rounded / (2**7 - 1) * (SF8_MAX - SF8_MIN) + SF8_MIN\n",
    "    \n",
    "    return quantized\n",
    "\n",
    "def apply_selective_sf8_quantization(model):\n",
    "    total_params = 0\n",
    "    quantized_params = 0\n",
    "    memory_usage = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if is_in_sf8_range(param.data):\n",
    "            param.data = quantize_to_sf8(param.data)\n",
    "            quantized_params += param.numel()\n",
    "            memory_usage += param.numel() * 8 / 8  # 8 bits per parameter\n",
    "        else:\n",
    "            memory_usage += param.numel() * 32 / 8  # 32 bits for non-quantized parameters\n",
    "\n",
    "    memory_usage_mb = memory_usage / (1024 * 1024)\n",
    "    return total_params, quantized_params, memory_usage_mb\n",
    "\n",
    "def verify_selective_sf8_quantization(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if is_in_sf8_range(param.data):\n",
    "            unique_values = torch.unique(param)\n",
    "            if len(unique_values) > 255:\n",
    "                print(f\"Warning: Quantized parameter {name} has more than 255 unique values\")\n",
    "            else:\n",
    "                print(f\"Layer {name} is correctly quantized to SF8\")\n",
    "        else:\n",
    "            print(f\"Layer {name} is not quantized (outside SF8 range)\")\n",
    "\n",
    "# Usage in your main script\n",
    "model_name = '/kaggle/working/yolov7/yolov7-d6.pt'\n",
    "sf16_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\n",
    "sf16_model.eval()\n",
    "total_params, quantized_params, quantized_memory = apply_selective_sf16_quantization(sf16_model)\n",
    "\n",
    "print(\"SF16 Quantized\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Quantized parameters: {quantized_params}\")\n",
    "print(f\"Quantization percentage: {quantized_params / total_params * 100:.2f}%\")\n",
    "print(f\"Estimated memory usage: {quantized_memory:.2f} MB\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "sf8_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\n",
    "sf8_model.eval()\n",
    "total_params, quantized_params, quantized_memory = apply_selective_sf8_quantization(sf8_model)\n",
    "\n",
    "print(\"SF8 Quantized\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Quantized parameters: {quantized_params}\")\n",
    "print(f\"Quantization percentage: {quantized_params / total_params * 100:.2f}%\")\n",
    "print(f\"Estimated memory usage: {quantized_memory:.2f} MB\")\n",
    "\n",
    "# Verify quantization\n",
    "# print(\"\\nVerifying Selective SF16 Quantization:\")\n",
    "# verify_selective_sf16_quantization(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN BENCHMARK COMPARISONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image\n",
    "!wget https://upload.wikimedia.org/wikipedia/commons/6/64/Cat_and_dog_standoff_%283926784260%29.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import psutil\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "def calculate_memory_fp32(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    memory_usage_fp32 = total_params * 4 / (1024 * 1024)\n",
    "    return memory_usage_fp32\n",
    "\n",
    "def estimate_complexity(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def detect_image(model, image_path, is_sf16=False, is_sf8=False):\n",
    "    \n",
    "    process = psutil.Process()\n",
    "    initial_memory = process.memory_info().rss\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (640, 640))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = model(img)\n",
    "    end_time = time.time()\n",
    "    pred_boxes = results.pandas().xyxy[0]  # Get predictions\n",
    "\n",
    "    complexity = estimate_complexity(model)\n",
    "\n",
    "    final_memory = process.memory_info().rss\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_used = (final_memory - initial_memory) / (1024 * 1024)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    return {\n",
    "        \"time\": elapsed_time,\n",
    "        \"total_params\": total_params,\n",
    "        \"complexity\": complexity,\n",
    "        \"pred_boxes\": pred_boxes  # Return predictions\n",
    "    }\n",
    "\n",
    "def compare_predictions(pred_boxes_normal, pred_boxes_quantized):\n",
    "    pred_normal_tensor = torch.tensor(pred_boxes_normal[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype=torch.float32)\n",
    "    pred_quantized_tensor = torch.tensor(pred_boxes_quantized[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype=torch.float32)\n",
    "\n",
    "    # Calculate IoU\n",
    "    ious = box_iou(pred_normal_tensor, pred_quantized_tensor)\n",
    "    \n",
    "    # Calculate mean IoU\n",
    "    mean_iou = ious.mean().item() if ious.numel() > 0 else 0\n",
    "\n",
    "    # Calculate distance between boxes\n",
    "    distances = []\n",
    "    for normal_box in pred_boxes_normal[['xmin', 'ymin', 'xmax', 'ymax']].values:\n",
    "        closest_distance = float('inf')\n",
    "        for quantized_box in pred_boxes_quantized[['xmin', 'ymin', 'xmax', 'ymax']].values:\n",
    "            distance = np.linalg.norm(normal_box - quantized_box)\n",
    "            closest_distance = min(closest_distance, distance)\n",
    "        distances.append(closest_distance)\n",
    "    \n",
    "    mean_distance = np.mean(distances)\n",
    "\n",
    "    return {\n",
    "        \"mean_iou\": mean_iou,\n",
    "        \"mean_distance\": mean_distance,\n",
    "        \"num_predictions_normal\": len(pred_boxes_normal),\n",
    "        \"num_predictions_quantized\": len(pred_boxes_quantized)\n",
    "    }\n",
    "\n",
    "# Path to your image\n",
    "image_path = '/kaggle/working/Cat_and_dog_standoff_(3926784260).jpg'\n",
    "\n",
    "# Load models\n",
    "normal_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\n",
    "normal_model.eval()\n",
    "normal_results = detect_image(normal_model, image_path)\n",
    "memory_model = calculate_memory_fp32(model)\n",
    "print(f\"Normal Predictions: {normal_results['pred_boxes']}\")\n",
    "\n",
    "sf16_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\n",
    "sf16_model.eval()\n",
    "total_params, sf16_params, sf16_memory = apply_selective_sf16_quantization(sf16_model)\n",
    "sf16_results = detect_image(sf16_model, image_path, is_sf16=True)\n",
    "print(f\"SF16 Quantized Predictions: {sf16_results['pred_boxes']}\")\n",
    "\n",
    "# Compare sf16 predictions\n",
    "sf16_comparison_results = compare_predictions(normal_results['pred_boxes'], sf16_results['pred_boxes'])\n",
    "\n",
    "sf8_model = torch.hub.load('yolov7', 'custom', model_name, source='local', force_reload=True)\n",
    "sf8_model.eval()\n",
    "total_params, sf8_params, sf8_memory = apply_selective_sf8_quantization(sf8_model)\n",
    "sf8_results = detect_image(sf8_model, image_path, is_sf8=True)\n",
    "print(f\"SF8 Quantized Predictions: {sf8_results['pred_boxes']}\")\n",
    "\n",
    "# Compare sf8 predictions\n",
    "sf8_comparison_results = compare_predictions(normal_results['pred_boxes'], sf8_results['pred_boxes'])\n",
    "\n",
    "# Print comparison results\n",
    "print(\"Normal YOLOv7:\")\n",
    "print(f\"Processing time: {normal_results['time']:.2f} seconds\")\n",
    "print(f\"Memory used by model: {memory_model:.2} MB\")\n",
    "print(f\"Estimated complexity: {normal_results['complexity']:,} operations\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "print(\"\\nSelectively Quantized YOLOv7 (SF16):\")\n",
    "print(f\"Processing time: {sf16_results['time']:.2f} seconds\")\n",
    "print(f\"Memory used by model: {sf16_memory:.2f} MB\")\n",
    "print(f\"Estimated complexity: {sf16_results['complexity']:,} operations\")\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Quantized parameters: {sf16_params}\")\n",
    "print(f\"Percentage of parameters quantized: {(sf16_params / total_params) * 100:.2f}%\")\n",
    "\n",
    "sf16_time_diff = (normal_results['time'] - sf16_results['time']) / normal_results['time'] * 100\n",
    "sf16_memory_diff = (memory_model - sf16_memory) / memory_model * 100\n",
    "\n",
    "print(f\"\\nTime improvement: {sf16_time_diff:.2f}%\")\n",
    "print(f\"Memory improvement: {sf16_memory_diff:.2f}%\")\n",
    "\n",
    "print(\"Comparison Results:\")\n",
    "print(f\"Mean IoU: {100*sf16_comparison_results['mean_iou']:.2f}%\")\n",
    "print(f\"Mean Distance between predictions: {sf16_comparison_results['mean_distance']:.2f}\")\n",
    "print(f\"Number of predictions (Normal): {sf16_comparison_results['num_predictions_normal']}\")\n",
    "print(f\"Number of predictions (Quantized): {sf16_comparison_results['num_predictions_quantized']}\")\n",
    "\n",
    "print(\"\\nSelectively Quantized YOLOv7 (SF8):\")\n",
    "print(f\"Processing time: {sf8_results['time']:.2f} seconds\")\n",
    "print(f\"Memory used by model: {sf8_memory:.2f} MB\")\n",
    "print(f\"Estimated complexity: {sf8_results['complexity']:,} operations\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Quantized parameters: {sf8_params:,}\")\n",
    "print(f\"Percentage of parameters quantized: {(sf8_params / total_params) * 100:.2f}%\")\n",
    "\n",
    "sf8_time_diff = (normal_results['time'] - sf8_results['time']) / normal_results['time'] * 100\n",
    "sf8_memory_diff = (memory_model- sf8_memory) / memory_model * 100\n",
    "\n",
    "print(f\"\\nTime improvement: {sf8_time_diff:.2f}%\")\n",
    "print(f\"Memory improvement: {sf8_memory_diff:.2f}%\")\n",
    "\n",
    "print(\"Comparison Results:\")\n",
    "print(f\"Mean IoU: {100*sf8_comparison_results['mean_iou']:.2f}%\")\n",
    "print(f\"Mean Distance between predictions: {sf8_comparison_results['mean_distance']:.2f}\")\n",
    "print(f\"Number of predictions (Normal): {sf8_comparison_results['num_predictions_normal']}\")\n",
    "print(f\"Number of predictions (Quantized): {sf8_comparison_results['num_predictions_quantized']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING ON LARGE LANGUAGE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T19:28:09.591367Z",
     "iopub.status.busy": "2024-10-16T19:28:09.590721Z",
     "iopub.status.idle": "2024-10-16T19:28:34.891984Z",
     "shell.execute_reply": "2024-10-16T19:28:34.890927Z",
     "shell.execute_reply.started": "2024-10-16T19:28:09.591333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.42.3\n",
      "    Uninstalling transformers-4.42.3:\n",
      "      Successfully uninstalled transformers-4.42.3\n",
      "Successfully installed tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T19:28:34.894970Z",
     "iopub.status.busy": "2024-10-16T19:28:34.894650Z",
     "iopub.status.idle": "2024-10-16T19:32:10.013287Z",
     "shell.execute_reply": "2024-10-16T19:32:10.010740Z",
     "shell.execute_reply.started": "2024-10-16T19:28:34.894938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12fe333d3cc47e8929186fba7656862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae56a827c93e4d5986c0f96c195d2145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837edac9f0524ea3b342c0edde9a31c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f931241f4849c6a28a9af89d23a853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normal model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1a1c6cf5bf4d19a4b848e36c25a9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a8bc33950049d68ac652f7c99a9b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9495469019c948ec8902c3da5ffe2667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5694414a8a4e7d888660d51d179afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5d143aab474bccb2180bcaaa212b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29d8663f6d440089528c3794ef9d8b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc27bd84aef42d1b1475cd682779294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39493ed91164dda9105e42cc98e6db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and quantizing SF16 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff3146099c1475da326e01ba171a4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and quantizing SF8 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6cfcd5b10547c7976854587852d77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating normal model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SF16 quantized model...\n",
      "\n",
      "Evaluating SF8 quantized model...\n",
      "\n",
      "Normal Gemma-2:\n",
      "Processing time: 27.07 seconds\n",
      "Memory used by model: 9972.92 MB\n",
      "Estimated complexity: 2,614,341,888 operations\n",
      "Generated text: Near them, on the sand, Half sunk a shattered visage lies, whose frown, And wrinkled lip, and sneer of cold command, Tell that its sculptor well those passions read Which yet survive, stamped on these lifeless things, The hand that mocked them, and the heart that fed.\n",
      "\n",
      "The hand that mocked them,\n",
      "\n",
      "SF16 Quantized Gemma-2:\n",
      "Processing time: 25.38 seconds\n",
      "Memory used by model: 6111.86 MB\n",
      "Estimated complexity: 2,614,341,888 operations\n",
      "Total parameters: 2,614,341,888\n",
      "Quantized parameters: 2,024,308,224\n",
      "Percentage of parameters quantized: 77.43%\n",
      "Generated text: Near them, on the sand, Half sunk a shattered visage lies, whose frown, And wrinkled lip, and sneer of cold command, Tell that its sculptor well those passions read Which yet survive, stamped on these lifeless things, The hand that mocked them, and the heart that fed.\n",
      "\n",
      "The hand that mocked them,\n",
      "\n",
      "Time improvement: 6.25%\n",
      "Memory improvement: 38.72%\n",
      "\n",
      "SF8 Gemma-2:\n",
      "Processing time: 24.57 seconds\n",
      "Memory used by model: 4181.33 MB\n",
      "Estimated complexity: 2,614,341,888 operations\n",
      "Total parameters: 2,614,341,888\n",
      "Quantized parameters: 2,024,308,224\n",
      "Percentage of parameters quantized: 77.43%\n",
      "Generated text: Near them, on the sand, Half sunk a shattered visage lies, UST->___x kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan kasarigan\n",
      "\n",
      "Time improvement: 9.23%\n",
      "Memory improvement: 58.07%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Constants for SF16 and SF8\n",
    "SF16_MAX = 0.999969482421875\n",
    "SF16_MIN = -0.999969482421875\n",
    "SF8_MAX = 0.9921875\n",
    "SF8_MIN = -0.9921875\n",
    "\n",
    "def is_in_sf16_range(tensor):\n",
    "    return torch.all((tensor >= SF16_MIN) & (tensor <= SF16_MAX))\n",
    "\n",
    "def is_in_sf8_range(tensor):\n",
    "    return torch.all((tensor >= SF8_MIN) & (tensor <= SF8_MAX))\n",
    "\n",
    "def quantize_to_sf16(tensor):\n",
    "    tensor = torch.clamp(tensor, SF16_MIN, SF16_MAX)\n",
    "    scaled = (tensor - SF16_MIN) / (SF16_MAX - SF16_MIN) * (2**15 - 1)\n",
    "    rounded = torch.round(scaled)\n",
    "    quantized = rounded / (2**15 - 1) * (SF16_MAX - SF16_MIN) + SF16_MIN\n",
    "    return quantized\n",
    "\n",
    "def quantize_to_sf8(tensor):\n",
    "    tensor = torch.clamp(tensor, SF8_MIN, SF8_MAX)\n",
    "    scaled = (tensor - SF8_MIN) / (SF8_MAX - SF8_MIN) * (2**7 - 1)\n",
    "    rounded = torch.round(scaled)\n",
    "    quantized = rounded / (2**7 - 1) * (SF8_MAX - SF8_MIN) + SF8_MIN\n",
    "    return quantized\n",
    "\n",
    "def apply_selective_quantization(model, quantize_func, is_in_range_func):\n",
    "    total_params = 0\n",
    "    quantized_params = 0\n",
    "    memory_usage = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if is_in_range_func(param.data):\n",
    "            param.data = quantize_func(param.data)\n",
    "            quantized_params += param.numel()\n",
    "            memory_usage += param.numel() * (16 if quantize_func == quantize_to_sf16 else 8) / 8\n",
    "        else:\n",
    "            memory_usage += param.numel() * 32 / 8\n",
    "\n",
    "    memory_usage_mb = memory_usage / (1024 * 1024)\n",
    "    return total_params, quantized_params, memory_usage_mb\n",
    "\n",
    "def calculate_memory_fp32(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    memory_usage_fp32 = total_params * 4 / (1024 * 1024)\n",
    "    return memory_usage_fp32\n",
    "\n",
    "def estimate_complexity(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def evaluate_model(model, tokenizer, prompt):\n",
    "    process = psutil.Process()\n",
    "    initial_memory = process.memory_info().rss\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    final_memory = process.memory_info().rss\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_used = (final_memory - initial_memory) / (1024 * 1024)\n",
    "\n",
    "    complexity = estimate_complexity(model)\n",
    "\n",
    "    return {\n",
    "        \"time\": elapsed_time,\n",
    "        \"memory_used\": memory_used,\n",
    "        \"complexity\": complexity,\n",
    "        \"generated_text\": generated_text\n",
    "    }\n",
    "\n",
    "# Load models\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading normal model...\")\n",
    "normal_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "normal_model.eval()\n",
    "\n",
    "print(\"Loading and quantizing SF16 model...\")\n",
    "sf16_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "sf16_model.eval()\n",
    "total_params, sf16_params, sf16_memory = apply_selective_quantization(sf16_model, quantize_to_sf16, is_in_sf16_range)\n",
    "\n",
    "print(\"Loading and quantizing SF8 model...\")\n",
    "sf8_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "sf8_model.eval()\n",
    "_, sf8_params, sf8_memory = apply_selective_quantization(sf8_model, quantize_to_sf8, is_in_sf8_range)\n",
    "\n",
    "# Evaluation\n",
    "prompt = \"Near them, on the sand, Half sunk a shattered visage lies,\"\n",
    "\n",
    "print(\"\\nEvaluating normal model...\")\n",
    "normal_results = evaluate_model(normal_model, tokenizer, prompt)\n",
    "memory_model = calculate_memory_fp32(normal_model)\n",
    "\n",
    "print(\"\\nEvaluating SF16 quantized model...\")\n",
    "sf16_results = evaluate_model(sf16_model, tokenizer, prompt)\n",
    "\n",
    "print(\"\\nEvaluating SF8 quantized model...\")\n",
    "sf8_results = evaluate_model(sf8_model, tokenizer, prompt)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nNormal Gemma-2:\")\n",
    "print(f\"Processing time: {normal_results['time']:.2f} seconds\")\n",
    "print(f\"Memory used by model: {memory_model:.2f} MB\")\n",
    "print(f\"Estimated complexity: {normal_results['complexity']:,} operations\")\n",
    "print(f\"Generated text: {normal_results['generated_text']}\")\n",
    "\n",
    "print(\"\\nSF16 Quantized Gemma-2:\")\n",
    "print(f\"Processing time: {sf16_results['time']:.2f} seconds\")\n",
    "print(f\"Memory used by model: {sf16_memory:.2f} MB\")\n",
    "print(f\"Estimated complexity: {sf16_results['complexity']:,} operations\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Quantized parameters: {sf16_params:,}\")\n",
    "print(f\"Percentage of parameters quantized: {(sf16_params / total_params) * 100:.2f}%\")\n",
    "print(f\"Generated text: {sf16_results['generated_text']}\")\n",
    "\n",
    "sf16_time_diff = (normal_results['time'] - sf16_results['time']) / normal_results['time'] * 100\n",
    "sf16_memory_diff = (memory_model - sf16_memory) / memory_model * 100\n",
    "\n",
    "print(f\"\\nTime improvement: {sf16_time_diff:.2f}%\")\n",
    "print(f\"Memory improvement: {sf16_memory_diff:.2f}%\")\n",
    "\n",
    "print(\"\\nSF8 Gemma-2:\")\n",
    "print(f\"Processing time: {sf8_results['time']:.2f} seconds\")\n",
    "print(f\"Memory used by model: {sf8_memory:.2f} MB\")\n",
    "print(f\"Estimated complexity: {sf8_results['complexity']:,} operations\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Quantized parameters: {sf8_params:,}\")\n",
    "print(f\"Percentage of parameters quantized: {(sf8_params / total_params) * 100:.2f}%\")\n",
    "print(f\"Generated text: {sf8_results['generated_text']}\")\n",
    "\n",
    "sf8_time_diff = (normal_results['time'] - sf8_results['time']) / normal_results['time'] * 100\n",
    "sf8_memory_diff = (memory_model - sf8_memory) / memory_model * 100\n",
    "\n",
    "print(f\"\\nTime improvement: {sf8_time_diff:.2f}%\")\n",
    "print(f\"Memory improvement: {sf8_memory_diff:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = SuperFloat(binary=0100000000000000, float=0.5)\n",
      "b = SuperFloat(binary=1010000000000000, float=-0.25)\n",
      "a + b = SuperFloat(binary=0010000000000000, float=0.25)\n",
      "a - b = SuperFloat(binary=0110000000000000, float=0.75)\n",
      "a * b = SuperFloat(binary=1001000000000000, float=-0.125)\n",
      "a / b = SuperFloat(binary=1111111111111111, float=-0.999969482421875)\n"
     ]
    }
   ],
   "source": [
    "class SuperFloat:\n",
    "    def __init__(self, binary_repr):\n",
    "        if len(binary_repr) != 16 or any(bit not in {'0', '1'} for bit in binary_repr):\n",
    "            raise ValueError(\"Binary representation must be a 16-bit binary string.\")\n",
    "        self.binary_repr = binary_repr\n",
    "\n",
    "    @property\n",
    "    def sign_bit(self):\n",
    "        return self.binary_repr[0]\n",
    "\n",
    "    @property\n",
    "    def mantissa_bits(self):\n",
    "        return self.binary_repr[1:]\n",
    "\n",
    "    def to_float(self):\n",
    "        # Convert binary mantissa to fractional value\n",
    "        mantissa = int(self.mantissa_bits, 2) / (1 << 15)\n",
    "        return -mantissa if self.sign_bit == '1' else mantissa\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, value):\n",
    "        if not (-1 < value < 1):\n",
    "            raise ValueError(\"Value must be between -1 and 1 (exclusive).\")\n",
    "        sign_bit = '1' if value < 0 else '0'\n",
    "        mantissa = int(abs(value) * (1 << 15))\n",
    "        mantissa_bits = f\"{mantissa:015b}\"\n",
    "        return cls(sign_bit + mantissa_bits)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Convert to floats, perform addition, and clamp to valid range\n",
    "        result = self.to_float() + other.to_float()\n",
    "        if result >= 1:\n",
    "            result = 1 - 1 / (1 << 15)\n",
    "        elif result <= -1:\n",
    "            result = -1 + 1 / (1 << 15)\n",
    "        return SuperFloat.from_float(result)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # Convert to floats, perform subtraction, and clamp to valid range\n",
    "        result = self.to_float() - other.to_float()\n",
    "        if result >= 1:\n",
    "            result = 1 - 1 / (1 << 15)\n",
    "        elif result <= -1:\n",
    "            result = -1 + 1 / (1 << 15)\n",
    "        return SuperFloat.from_float(result)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Convert to floats, perform multiplication, and clamp to valid range\n",
    "        result = self.to_float() * other.to_float()\n",
    "        return SuperFloat.from_float(result)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if other.to_float() == 0:\n",
    "            raise ZeroDivisionError(\"Division by zero is not allowed.\")\n",
    "        # Convert to floats, perform division, and clamp to valid range\n",
    "        result = self.to_float() / other.to_float()\n",
    "        if result >= 1:\n",
    "            result = 1 - 1 / (1 << 15)\n",
    "        elif result <= -1:\n",
    "            result = -1 + 1 / (1 << 15)\n",
    "        return SuperFloat.from_float(result)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"SuperFloat(binary={self.binary_repr}, float={self.to_float()})\"\n",
    "\n",
    "# Example usage\n",
    "a = SuperFloat.from_float(0.5)\n",
    "b = SuperFloat.from_float(-0.25)\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "\n",
    "# Addition\n",
    "c = a + b\n",
    "print(f\"a + b = {c}\")\n",
    "\n",
    "# Subtraction\n",
    "d = a - b\n",
    "print(f\"a - b = {d}\")\n",
    "\n",
    "# Multiplication\n",
    "e = a * b\n",
    "print(f\"a * b = {e}\")\n",
    "\n",
    "# Division\n",
    "f = a / b\n",
    "print(f\"a / b = {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Set device: GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom SuperFloat quantization with variable bitwidth\n",
    "def superfloat_quantize(tensor, bitwidth):\n",
    "    if bitwidth < 1 or bitwidth > 16:\n",
    "        raise ValueError(\"Bitwidth must be between 1 and 16.\")\n",
    "    \n",
    "    # Calculate the number of mantissa bits\n",
    "    mantissa_bits = bitwidth - 1  # 1 bit for the sign\n",
    "    max_value = (1 << mantissa_bits) - 1  # Maximum representable integer\n",
    "    \n",
    "    # Clamp values to the range (-1, 1)\n",
    "    tensor = torch.clamp(tensor, -1 + 1e-7, 1 - 1e-7)\n",
    "    \n",
    "    # Scale to the representable range and round\n",
    "    tensor = torch.round(tensor * max_value) / max_value\n",
    "    return tensor\n",
    "\n",
    "# Quantize model weights\n",
    "def quantize_model_weights(model, bitwidth):\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"weight\" in name:  # Only quantize weights, not biases\n",
    "                param.data = superfloat_quantize(param.data, bitwidth)\n",
    "    return model\n",
    "\n",
    "# Calculate perplexity\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Move input tensors to the GPU (if available)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return torch.exp(loss).item()\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "model_name = \"openai-community/gpt2-large\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)  # Move model to GPU\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Original model perplexity\n",
    "text = \"Hello, how are you?\"\n",
    "original_perplexity = calculate_perplexity(model, tokenizer, text)\n",
    "print(f\"Original Model Perplexity: {original_perplexity}\")\n",
    "\n",
    "# Quantize model weights with variable bitwidth\n",
    "bitwidth = 16  # Example: 16-bit quantization\n",
    "quantized_model = quantize_model_weights(model, bitwidth)\n",
    "\n",
    "# Quantized model perplexity\n",
    "quantized_perplexity = calculate_perplexity(quantized_model, tokenizer, text)\n",
    "print(f\"{bitwidth}-bit Quantized Model Perplexity: {quantized_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5809117,
     "sourceId": 9537222,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5863008,
     "sourceId": 9608917,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
